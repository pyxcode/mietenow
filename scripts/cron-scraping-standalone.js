#!/usr/bin/env node

/**
 * Script de scraping standalone qui ne dÃ©pend pas du serveur Next.js
 * Utilise directement les scrapers sans passer par l'API
 */

const mongoose = require('mongoose')
const fs = require('fs')
const path = require('path')
require('dotenv').config({ path: '.env.local' })

// Importer les scrapers directement
const { ScraperManager } = require('../lib/scrapers/core/scraper-manager.js')

const LOG_DIR = path.join(process.cwd(), 'logs')
const LOG_FILE = path.join(LOG_DIR, 'cron-scraping-standalone.log')

// CrÃ©er le dossier de logs s'il n'existe pas
if (!fs.existsSync(LOG_DIR)) {
  fs.mkdirSync(LOG_DIR, { recursive: true })
}

function log(message) {
  const timestamp = new Date().toISOString()
  const logMessage = `[${timestamp}] ${message}\n`
  
  console.log(logMessage.trim())
  fs.appendFileSync(LOG_FILE, logMessage)
}

async function connectDB() {
  try {
    const mongoUri = process.env.MONGODB_URI
    if (!mongoUri) {
      throw new Error('MONGODB_URI not found in environment variables')
    }
    
    const baseUri = mongoUri.endsWith('/') ? mongoUri.slice(0, -1) : mongoUri
    const fullUri = `${baseUri}/mietenow-prod`
    
    await mongoose.connect(fullUri)
    log('âœ… ConnectÃ© Ã  MongoDB')
  } catch (error) {
    log(`âŒ Erreur de connexion MongoDB: ${error.message}`)
    throw error
  }
}

async function checkListingStatuses() {
  try {
    log('ğŸ” VÃ©rification du statut des annonces...')
    
    // Importer le modÃ¨le Listing
    const Listing = require('./models/Listing.js')
    
    // RÃ©cupÃ©rer toutes les annonces actives
    const listings = await Listing.find({ 
      active: { $ne: false } 
    }).limit(50) // Limiter pour Ã©viter de surcharger
    
    const results = {
      total: listings.length,
      checked: 0,
      removed: 0,
      errors: 0
    }
    
    // VÃ©rifier chaque annonce
    for (const listing of listings) {
      try {
        results.checked++
        
        // VÃ©rifier si le lien est accessible
        const controller = new AbortController()
        const timeoutId = setTimeout(() => controller.abort(), 10000)
        
        const response = await fetch(listing.link, {
          method: 'HEAD',
          signal: controller.signal,
          headers: {
            'User-Agent': 'Mozilla/5.0 (compatible; MieteNow/1.0)'
          }
        })
        
        clearTimeout(timeoutId)
        
        if (!response.ok) {
          // Marquer l'annonce comme inactive
          await Listing.findByIdAndUpdate(listing._id, { 
            active: false,
            status_checked_at: new Date(),
            status_error: `HTTP ${response.status}`
          })
          
          results.removed++
          log(`âŒ Annonce ${listing._id} dÃ©sactivÃ©e: HTTP ${response.status}`)
        } else {
          // Mettre Ã  jour la date de vÃ©rification
          await Listing.findByIdAndUpdate(listing._id, { 
            status_checked_at: new Date(),
            status_error: null
          })
          
          log(`âœ… Annonce ${listing._id} vÃ©rifiÃ©e: HTTP ${response.status}`)
        }
        
        // Pause entre les vÃ©rifications
        await new Promise(resolve => setTimeout(resolve, 1000))
        
      } catch (error) {
        results.errors++
        log(`âŒ Erreur lors de la vÃ©rification de l'annonce ${listing._id}: ${error.message}`)
      }
    }
    
    log(`ğŸ“Š VÃ©rification terminÃ©e: ${results.checked} vÃ©rifiÃ©es, ${results.removed} supprimÃ©es, ${results.errors} erreurs`)
    return results
    
  } catch (error) {
    log(`âŒ Erreur lors de la vÃ©rification des statuts: ${error.message}`)
    throw error
  }
}

async function runScraping() {
  try {
    log('ğŸš€ DÃ©marrage du scraping...')
    
    const manager = new ScraperManager()
    const results = await manager.scrapeAll()
    
    log(`âœ… Scraping terminÃ©: ${JSON.stringify(results)}`)
    return results
    
  } catch (error) {
    log(`âŒ Erreur lors du scraping: ${error.message}`)
    throw error
  }
}

async function cleanupOldListings() {
  try {
    log('ğŸ§¹ Nettoyage des anciennes annonces...')
    
    const Listing = require('./models/Listing.js')
    
    // Supprimer les annonces plus anciennes que 30 jours
    const thirtyDaysAgo = new Date()
    thirtyDaysAgo.setDate(thirtyDaysAgo.getDate() - 30)
    
    const result = await Listing.deleteMany({
      created_at: { $lt: thirtyDaysAgo }
    })
    
    log(`ğŸ—‘ï¸ ${result.deletedCount} anciennes annonces supprimÃ©es`)
    return result.deletedCount
    
  } catch (error) {
    log(`âŒ Erreur lors du nettoyage: ${error.message}`)
    throw error
  }
}

async function main() {
  const startTime = new Date()
  log(`ğŸš€ DÃ©but du cron de scraping standalone - ${startTime.toISOString()}`)
  
  try {
    await connectDB()
    
    // 1. VÃ©rifier les statuts des annonces
    const statusResults = await checkListingStatuses()
    
    // 2. Lancer le scraping
    const scrapingResults = await runScraping()
    
    // 3. Nettoyer les anciennes annonces
    const cleanupCount = await cleanupOldListings()
    
    const endTime = new Date()
    const duration = endTime.getTime() - startTime.getTime()
    
    log(`ğŸ‰ Cron terminÃ© avec succÃ¨s en ${duration}ms`)
    log(`ğŸ“Š RÃ©sumÃ©: ${statusResults.checked} vÃ©rifiÃ©es, ${statusResults.removed} supprimÃ©es, ${cleanupCount} anciennes supprimÃ©es`)
    
  } catch (error) {
    log(`âŒ Erreur fatale: ${error.message}`)
    process.exit(1)
  } finally {
    await mongoose.disconnect()
    log('âœ… DÃ©connectÃ© de MongoDB')
  }
}

// ExÃ©cuter le script
main().catch(console.error)
